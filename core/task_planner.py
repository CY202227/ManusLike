"""
ä»»åŠ¡è§„åˆ’å™¨
è´Ÿè´£åˆ†æç”¨æˆ·éœ€æ±‚å¹¶ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
"""

import asyncio
import json
import logging
from typing import Dict, Any

from openai import OpenAI
from .models import (
    TaskPlan, Plan, Step, TaskStatus, TaskType, 
    TaskNeedClarification, TaskClarityScore,IsTaskOrConversation
)
from .event_emitter import ExecutionEventEmitter

logger = logging.getLogger(__name__)


class TaskClarityAnalyzer:
    """ä»»åŠ¡æ˜ç¡®åº¦åˆ†æå™¨"""
    
    def __init__(self, llm_client: OpenAI, model_name: str = "Qwen-72B"):
        self.llm_client = llm_client
        self.model_name = model_name
        self.stream_callback = None  # æµå¼è¾“å‡ºå›è°ƒå‡½æ•°
    
    async def analyze_clarity(self, user_input: str) -> TaskClarityScore:
        """åˆ†æä»»åŠ¡æ˜ç¡®åº¦"""
        clarity_prompt = """
# è§’è‰²ï¼š
ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡åˆ†æä¸“å®¶ï¼Œéœ€è¦åˆ¤æ–­ç”¨æˆ·éœ€æ±‚æ˜¯å¦è¶³å¤Ÿæ˜ç¡®ã€‚

# ä»»åŠ¡ï¼š
è¯·åˆ†æç”¨æˆ·è¾“å…¥çš„ä»»åŠ¡æ˜ç¡®åº¦ï¼Œä»ä»¥ä¸‹ç»´åº¦è¯„åˆ†ï¼š

ç”¨æˆ·è¾“å…¥: {user_input}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ‡å‡†è¯„ä¼°ï¼š
1. clarity_score (0-10åˆ†): æ•´ä½“æ˜ç¡®åº¦è¯„åˆ†
2. has_clear_action: æ˜¯å¦åŒ…å«æ˜ç¡®çš„åŠ¨ä½œè¯ï¼ˆå¦‚ï¼šç”Ÿæˆã€åˆ›å»ºã€æœç´¢ã€è½¬æ¢ç­‰ï¼‰
3. has_sufficient_params: æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„å‚æ•°ä¿¡æ¯
4. is_simple_task: æ˜¯å¦ä¸ºç®€å•çš„å•æ­¥éª¤ä»»åŠ¡
5. needs_clarification: ç»¼åˆåˆ¤æ–­æ˜¯å¦éœ€è¦æ¾„æ¸…

è¯„åˆ†æ ‡å‡†ï¼š
- 9-10åˆ†: éå¸¸æ˜ç¡®ï¼ŒåŒ…å«å®Œæ•´ç›®æ ‡å’Œå‚æ•°
- 7-8åˆ†: æ¯”è¾ƒæ˜ç¡®ï¼Œå¯ä»¥ç›´æ¥æ‰§è¡Œ
- 5-6åˆ†: åŸºæœ¬æ˜ç¡®ï¼Œä½†å¯èƒ½éœ€è¦å°‘é‡æ¾„æ¸…
- 3-4åˆ†: ä¸å¤Ÿæ˜ç¡®ï¼Œéœ€è¦æ¾„æ¸…å…³é”®ä¿¡æ¯
- 0-2åˆ†: éå¸¸æ¨¡ç³Šï¼Œå¿…é¡»æ¾„æ¸…

ç¤ºä¾‹ï¼š
- "ç”Ÿæˆä¸€ä¸ªPython hello worldç¨‹åº" -> 9åˆ†ï¼Œä¸éœ€è¦æ¾„æ¸…
- "æœç´¢Pythonæ•™ç¨‹" -> 8åˆ†ï¼Œä¸éœ€è¦æ¾„æ¸…
- "ç”Ÿæˆä¸€ä¸ªç¨‹åº" -> 4åˆ†ï¼Œéœ€è¦æ¾„æ¸…
- "å¸®æˆ‘å¤„ç†æ–‡ä»¶" -> 2åˆ†ï¼Œéœ€è¦æ¾„æ¸…
"""
        
        try:
            messages = [
                {"role": "system", "content": clarity_prompt.format(user_input=user_input)}
            ]
            
            await self._stream_print("ğŸ” åˆ†æä»»åŠ¡æ˜ç¡®åº¦...")
            # æ·»åŠ æµå¼è¾“å‡º
            response = self.llm_client.beta.chat.completions.parse(
                model=self.model_name,
                messages=messages,
                temperature=0.1,
                response_format=TaskClarityScore
            )
            return response.choices[0].message.parsed
        
        except Exception as e:
            logger.error(f"ä»»åŠ¡æ˜ç¡®åº¦åˆ†æå¤±è´¥: {e}")
            # è¿”å›ä¿å®ˆçš„é»˜è®¤å€¼
            return TaskClarityScore(
                clarity_score=5,
                has_clear_action=False,
                has_sufficient_params=False,
                is_simple_task=False,
                needs_clarification=True
            )

    async def _stream_print(self, message: str = "", end: str = "\n"):
        """æµå¼è¾“å‡ºå‡½æ•°ï¼Œæ”¯æŒç»ˆç«¯å’ŒWebå‰ç«¯"""
        if self.stream_callback:
            # Webå‰ç«¯æ¨¡å¼ï¼šé€šè¿‡å›è°ƒå‘é€åˆ°å‰ç«¯
            await self.stream_callback(message + end)
        else:
            # ç»ˆç«¯æ¨¡å¼ï¼šç›´æ¥æ‰“å°
            print(message, end=end, flush=True)


# ========== ç³»ç»Ÿæç¤ºè¯ ==========

IMPROVED_REASON_SYSTEM_PROMPT = """
# è§’è‰²å®šä½
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»»åŠ¡åˆ†æåŠ©æ‰‹ï¼Œéœ€è¦åˆ¤æ–­ç”¨æˆ·éœ€æ±‚æ˜¯å¦è¶³å¤Ÿæ˜ç¡®ã€‚

# åˆ¤æ–­æ ‡å‡†
## ä¸éœ€è¦è¿½é—®çš„æƒ…å†µï¼ˆç›´æ¥è¿”å› need_clarification: falseï¼‰ï¼š
1. ç”¨æˆ·éœ€æ±‚å…·ä½“æ˜ç¡®ï¼ŒåŒ…å«å®Œæ•´çš„æ“ä½œç›®æ ‡
2. ç®€å•çš„å•æ­¥éª¤ä»»åŠ¡ï¼ˆå¦‚æœç´¢ã€ç”Ÿæˆç®€å•æ–‡ä»¶ç­‰ï¼‰  
3. ç”¨æˆ·ä½¿ç”¨äº†æ˜ç¡®çš„åŠ¨ä½œè¯ï¼ˆç”Ÿæˆã€åˆ›å»ºã€æœç´¢ã€è½¬æ¢ç­‰ï¼‰
4. å·²åŒ…å«å¿…è¦çš„å‚æ•°ä¿¡æ¯
5. ä»»åŠ¡ç›®æ ‡æ¸…æ™°ï¼Œå³ä½¿ç¼ºå°‘ç»†èŠ‚ä¹Ÿå¯ä»¥åˆç†æ¨æ–­

## éœ€è¦è¿½é—®çš„æƒ…å†µï¼š
1. ç”¨æˆ·éœ€æ±‚æ¨¡ç³Šæˆ–åŒ…å«å¤šä¸ªå¯èƒ½çš„è§£é‡Š
2. ç¼ºå°‘å…³é”®å‚æ•°ä¸”æ— æ³•åˆç†æ¨æ–­
3. å¤æ‚ä»»åŠ¡ä½†ç¼ºå°‘æ‰§è¡Œç»†èŠ‚
4. ç”¨æˆ·æ˜ç¡®è¡¨ç¤ºä¸ç¡®å®šæˆ–è¯¢é—®å»ºè®®

# ç¤ºä¾‹åˆ¤æ–­
- "ç”Ÿæˆä¸€ä¸ªPythonç¨‹åº" â†’ éœ€è¦è¿½é—®ï¼ˆç¼ºå°‘å…·ä½“åŠŸèƒ½ï¼‰
- "ç”Ÿæˆä¸€ä¸ªPython hello worldç¨‹åº" â†’ ä¸éœ€è¦è¿½é—®
- "ç”Ÿæˆä¸€ä¸ªç®€å•çš„Pythonæ¸¸æˆ" â†’ ä¸éœ€è¦è¿½é—®ï¼ˆå¯ä»¥æ¨æ–­ä¸ºç®€å•æ¸¸æˆï¼‰
- "å¸®æˆ‘æœç´¢" â†’ éœ€è¦è¿½é—®ï¼ˆç¼ºå°‘æœç´¢å†…å®¹ï¼‰
- "æœç´¢Pythonæ•™ç¨‹" â†’ ä¸éœ€è¦è¿½é—®
- "è½¬æ¢è¿™ä¸ªæ–‡ä»¶" â†’ éœ€è¦è¿½é—®ï¼ˆç¼ºå°‘æ–‡ä»¶è·¯å¾„å’Œç›®æ ‡æ ¼å¼ï¼‰
- "æŠŠPDFè½¬æˆWord" â†’ éœ€è¦è¿½é—®ï¼ˆç¼ºå°‘æ–‡ä»¶è·¯å¾„ï¼‰

# é‡è¦åŸåˆ™
- å€¾å‘äºä¸è¿½é—®ï¼Œé™¤éç¡®å®å¿…è¦
- ä¼˜å…ˆå°è¯•åˆç†æ¨æ–­ç”¨æˆ·æ„å›¾
- å¯¹äºæ˜ç¡®çš„åŠ¨ä½œ+ç®€å•æè¿°ï¼Œä¸è¦è¿½é—®
"""

PLAN_SYSTEM_PROMPT = """
# è§’è‰²ï¼š
ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡è§£å†³ä¸“å®¶ï¼Œä½ å¾ˆæ“…é•¿æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»“åˆå¯ç”¨çš„å·¥å…·ï¼ŒæŒ‰æ­¥éª¤åˆ¶å®šä¸€ä¸ªè§£å†³æ–¹æ¡ˆã€‚

# ä»»åŠ¡ï¼š
1. æ ¹æ®ç”¨æˆ·é—®é¢˜å’Œå¯ç”¨å·¥å…·ï¼Œåˆ¶å®šè¯¦ç»†çš„æ­¥éª¤åŒ–è§£å†³æ–¹æ¡ˆ
2. æ­¥éª¤è¦é€»è¾‘æ¸…æ™°ï¼Œèƒ½å¤Ÿå®Œç¾è§£å†³ç”¨æˆ·é—®é¢˜
3. åˆç†é€‰æ‹©å’Œç»„åˆå·¥å…·ï¼Œé¿å…ä¸å¿…è¦çš„æ­¥éª¤
4. ä½ éœ€è¦åˆç†ä½¿ç”¨æ–‡ä»¶ä¿å­˜å·¥å…·æ¥ä¿å­˜è¿‡ç¨‹ä¸­çš„æ–‡ä»¶,å¹¶ä½¿ç”¨æ–‡ä»¶ä¿å­˜å·¥å…·æ¥ä¿å­˜æœ€ç»ˆç»“æœ
5. æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼è¾“å‡ºè§£å†³æ–¹æ¡ˆ
6. å¦‚æœç”¨æˆ·çš„é—®é¢˜æ˜¯é—²èŠï¼Œè¯·è°ƒç”¨æ™®é€šå›å¤å·¥å…·'generate_answer_tool'æ¥è¿›è¡Œå›å¤,ç»“æœä¸éœ€è¦ä¿å­˜æˆæ–‡ä»¶
7. é™¤éç”¨æˆ·è¦æ±‚,å¦åˆ™å¿…é¡»ä½¿ç”¨ä¸­æ–‡æ¥è¿›è¡Œå›ç­”

# é‡è¦å·¥å…·ä½¿ç”¨è§„åˆ™ï¼š
- å½“ç”¨æˆ·è¦æ±‚"ç”Ÿæˆ"ã€"åˆ›å»º"ã€"å†™"ç¨‹åº/ä»£ç /æ–‡ä»¶æ—¶ï¼Œå¿…é¡»ä½¿ç”¨file_generation_toolæ¥åˆ›å»ºå®é™…æ–‡ä»¶
- å½“ç”¨æˆ·è¦æ±‚"æœç´¢"ã€"æŸ¥æ‰¾"ä¿¡æ¯æ—¶ï¼Œä½¿ç”¨web_search_tool
- å½“ç”¨æˆ·è¦æ±‚"è¯»å–"ã€"åˆ†æ"å·²æœ‰æ–‡ä»¶æ—¶ï¼Œä½¿ç”¨read_file_tool
- å½“ç”¨æˆ·è¦æ±‚ç”Ÿæˆå›¾ç‰‡æ—¶ï¼Œä½¿ç”¨image_generation_tool
- åªæœ‰åœ¨ç”¨æˆ·ä»…ä»…æ˜¯è¯¢é—®é—®é¢˜ã€éœ€è¦è§£é‡Šæˆ–é—²èŠæ—¶ï¼Œæ‰ä½¿ç”¨generate_answer_tool
- ä¸è¦ä»…ä»…ç”¨generate_answer_toolæ¥å›ç­”å…³äºå¦‚ä½•åˆ›å»ºæ–‡ä»¶çš„é—®é¢˜ï¼Œè€Œæ˜¯è°ƒç”¨åå†åˆ›å»ºæ–‡ä»¶

# å·¥å…·åˆ—è¡¨ï¼š
```
{tools}
```

# ä»»åŠ¡å¤æ‚åº¦åˆ†æï¼š
- simple: å•ä¸€å·¥å…·è°ƒç”¨å³å¯å®Œæˆ
- medium: éœ€è¦2-3ä¸ªæ­¥éª¤ï¼Œå·¥å…·ç»„åˆä½¿ç”¨
- complex: éœ€è¦å¤šæ­¥éª¤ï¼ŒåŒ…å«æ¡ä»¶åˆ¤æ–­å’Œæ•°æ®ä¼ é€’

# è¾“å‡ºæ ¼å¼ï¼š
```json
{output_format}
```

# æ³¨æ„äº‹é¡¹ï¼š
1. function_nameå¿…é¡»ä»å·¥å…·åˆ—è¡¨ä¸­é€‰æ‹©ï¼Œå¿…é¡»ä½¿ç”¨å®Œå…¨å‡†ç¡®çš„å·¥å…·åç§°
2. argså‚æ•°è¦ä¸å·¥å…·å®šä¹‰å®Œå…¨åŒ¹é…
3. step_descriptionè¦æ¸…æ™°æè¿°è¿™ä¸€æ­¥è¦åšä»€ä¹ˆ
4. æœ€åä¸€æ­¥è®¾ç½®is_finalä¸ºtrue
5. è€ƒè™‘æ­¥éª¤é—´çš„æ•°æ®ä¾èµ–å…³ç³»
6. ä¸è¦è¿›è¡Œä»»ä½•å·¥å…·åç§°æ˜ å°„æˆ–ä¿®æ”¹ï¼Œä¸¥æ ¼ä½¿ç”¨å·¥å…·åˆ—è¡¨ä¸­çš„åç§°
7. å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚ç”Ÿæˆæ–‡ä»¶æ—¶ï¼Œç›´æ¥ä½¿ç”¨file_generation_toolï¼Œä¸è¦ç”¨generate_answer_tool
"""


class TaskPlanner:
    """ä»»åŠ¡è§„åˆ’å™¨ - è´Ÿè´£åˆ†æç”¨æˆ·éœ€æ±‚å¹¶ç”Ÿæˆæ‰§è¡Œè®¡åˆ’"""
    
    def __init__(self, 
                 llm_client: OpenAI,
                 tool_manager,
                 model_name: str = "Qwen-72B",
                 event_emitter: ExecutionEventEmitter = None):
        """
        åˆå§‹åŒ–ä»»åŠ¡è§„åˆ’å™¨
        
        Args:
            llm_client: OpenAIå®¢æˆ·ç«¯
            tool_manager: å·¥å…·ç®¡ç†å™¨
            model_name: ä½¿ç”¨çš„LLMæ¨¡å‹åç§°
            event_emitter: äº‹ä»¶å‘å°„å™¨
        """
        self.llm_client = llm_client
        self.tool_manager = tool_manager
        self.model_name = model_name
        self.event_emitter = event_emitter or ExecutionEventEmitter()
        self.clarity_analyzer = TaskClarityAnalyzer(llm_client, model_name)
        self.stream_callback = None  # æµå¼è¾“å‡ºå›è°ƒå‡½æ•°
        self.last_completed_task = None  # æœ€åå®Œæˆçš„ä»»åŠ¡
        
        logger.info("TaskPlanneråˆå§‹åŒ–å®Œæˆ")
    
    def set_stream_callback(self, callback):
        """è®¾ç½®æµå¼è¾“å‡ºå›è°ƒå‡½æ•°"""
        self.stream_callback = callback
        self.clarity_analyzer.stream_callback = callback
    
    def set_last_completed_task(self, task_plan: 'TaskPlan'):
        """è®¾ç½®æœ€åå®Œæˆçš„ä»»åŠ¡ï¼Œç”¨äºæ£€æµ‹æ”¹è¿›è¯·æ±‚"""
        self.last_completed_task = task_plan
    
    async def _stream_print(self, message: str = "", end: str = "\n"):
        """æµå¼è¾“å‡ºå‡½æ•°ï¼Œæ”¯æŒç»ˆç«¯å’ŒWebå‰ç«¯"""
        if self.stream_callback:
            # Webå‰ç«¯æ¨¡å¼ï¼šé€šè¿‡å›è°ƒå‘é€åˆ°å‰ç«¯
            await self.stream_callback(message + end)
        else:
            # ç»ˆç«¯æ¨¡å¼ï¼šç›´æ¥æ‰“å°
            print(message, end=end, flush=True)
    
    async def _load_tools(self) -> None:
        """åŠ è½½æ‰€æœ‰å¯ç”¨å·¥å…·"""
        await self.tool_manager.load_all_tools()
    
    async def analyze_task(self, user_input: str) -> TaskPlan:
        """
        åˆ†æç”¨æˆ·ä»»åŠ¡ï¼Œç”Ÿæˆæ‰§è¡Œè®¡åˆ’
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„ä»»åŠ¡æè¿°
            
        Returns:
            TaskPlan: å®Œæ•´çš„ä»»åŠ¡è®¡åˆ’
        """
        logger.info(f"ğŸ“‹ å¼€å§‹åˆ†æä»»åŠ¡: {user_input[:100]}...")
        
        try:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºä»»åŠ¡æ”¹è¿›è¯·æ±‚
            if self.last_completed_task:
                is_improvement = await self._detect_task_improvement(user_input)
                if is_improvement:
                    logger.info("ğŸ”„ æ£€æµ‹åˆ°ä»»åŠ¡æ”¹è¿›è¯·æ±‚")
                    return await self._handle_task_improvement(user_input, self.last_completed_task)
            
            # å…¶æ¬¡æ£€æµ‹æ˜¯å¦ä¸ºå¯¹è¯è€Œéä»»åŠ¡
            is_conversation = await self._detect_conversation(user_input)
            if is_conversation:
                logger.info("ğŸ’¬ æ£€æµ‹åˆ°å¯¹è¯å†…å®¹ï¼Œç›´æ¥å›å¤")
                # åˆ›å»ºå¯¹è¯ç±»å‹çš„TaskPlanï¼ŒåŒ…å«ç›´æ¥å›å¤
                conversation_plan = TaskPlan(
                    user_input=user_input,
                    task_type="å¯¹è¯",
                    complexity_level="simple",
                    plan=Plan(steps=[
                        Step(
                            step_description="ç›´æ¥å¯¹è¯å›å¤",
                            function_name="chat_response",
                            args={"response": await self._generate_conversation_response(user_input)},
                            is_final=True
                        )
                    ]),
                    status=TaskStatus.PLANNING,
                    is_conversation=True  # æ ‡è®°ä¸ºå¯¹è¯
                )
                return conversation_plan
            
            # å‘å°„ä»»åŠ¡åˆ†æå¼€å§‹äº‹ä»¶
            if self.event_emitter:
                await self.event_emitter.emit_task_analysis_start(user_input)
                # å°å»¶è¿Ÿè®©å‰ç«¯çœ‹åˆ°äº‹ä»¶
                await asyncio.sleep(0.5)
            
            # ç¡®ä¿å·¥å…·å·²åŠ è½½
            await self._load_tools()
            
            # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨æ”¹è¿›çš„æ˜ç¡®åº¦åˆ†æ
            if self.event_emitter:
                await self.event_emitter.emit_clarity_check_start()
                # å°å»¶è¿Ÿè®©å‰ç«¯çœ‹åˆ°äº‹ä»¶
                await asyncio.sleep(0.3)
            
            clarity_result = await self.clarity_analyzer.analyze_clarity(user_input)
            logger.info(f"ğŸ“Š ä»»åŠ¡æ˜ç¡®åº¦è¯„åˆ†: {clarity_result.clarity_score}/10")
            
            # å‘å°„æ˜ç¡®åº¦è¯„åˆ†äº‹ä»¶
            if self.event_emitter:
                await self.event_emitter.emit_clarity_score(
                    clarity_result.clarity_score / 10.0,
                    clarity_result.needs_clarification,
                    getattr(clarity_result, 'questions', [])
                )
                # å°å»¶è¿Ÿè®©å‰ç«¯å¤„ç†
                await asyncio.sleep(0.5)
            
            # ç¬¬äºŒæ­¥ï¼šåŸºäºæ˜ç¡®åº¦å†³å®šæ˜¯å¦éœ€è¦æ¾„æ¸…
            if clarity_result.needs_clarification and clarity_result.clarity_score < 6:
                clarification_result = await self._analyze_requirements(user_input)
                if clarification_result.get("needs_clarification", False):
                    logger.info("â“ ä»»åŠ¡éœ€è¦æ¾„æ¸…")
                    
                    # å‘å°„éœ€è¦æ¾„æ¸…äº‹ä»¶
                    if self.event_emitter:
                        await self.event_emitter.emit_general_progress(
                            "clarification_needed",
                            "ä»»åŠ¡éœ€è¦è¿›ä¸€æ­¥æ¾„æ¸…"
                        )
                    
                    return TaskPlan(
                        user_input=user_input,
                        task_type="éœ€è¦æ¾„æ¸…",
                        complexity_level="unknown",
                        plan=Plan(steps=[]),
                        requires_clarification=True,
                        clarification_questions=clarification_result.get("questions", [])
                    )
            
            # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæ‰§è¡Œè®¡åˆ’
            logger.info("ğŸ”§ ç”Ÿæˆæ‰§è¡Œè®¡åˆ’...")
            
            # å…ˆåˆ†æä»»åŠ¡ç±»å‹å’Œå¤æ‚åº¦
            task_type = await self._analyze_task_type(user_input)
            if self.event_emitter:
                await self.event_emitter.emit_task_type_detected(task_type, 0.8)  # å‡è®¾80%ç½®ä¿¡åº¦
                await asyncio.sleep(0.3)
            
            # å‘å°„è®¡åˆ’ç”Ÿæˆå¼€å§‹äº‹ä»¶
            if self.event_emitter:
                await self.event_emitter.emit_plan_generation_start("unknown")  # å¤æ‚åº¦ç¨åç¡®å®š
                await asyncio.sleep(0.5)
            
            plan = await self._generate_plan(user_input)
            
            # å‘å°„æ¯ä¸ªæ­¥éª¤ç”Ÿæˆäº‹ä»¶ï¼ˆå¢åŠ å»¶è¿Ÿï¼‰
            if self.event_emitter:
                for i, step in enumerate(plan.steps):
                    await self.event_emitter.emit_plan_step_generated(
                        i, len(plan.steps), step.step_description, step.function_name
                    )
                    # æ¯ä¸ªæ­¥éª¤ä¹‹é—´ç¨å¾®å»¶è¿Ÿï¼Œè®©å‰ç«¯èƒ½çœ‹åˆ°é€ä¸ªç”Ÿæˆ
                    await asyncio.sleep(0.2)
            
            # ç¬¬å››æ­¥ï¼šéªŒè¯è®¡åˆ’ä¸­çš„å·¥å…·ï¼ˆä¸ä¿®æ”¹ï¼ŒåªéªŒè¯ï¼‰
            if self.event_emitter:
                await self.event_emitter.emit_general_progress(
                    "plan_validation",
                    "éªŒè¯è®¡åˆ’ä¸­çš„å·¥å…·å¯ç”¨æ€§..."
                )
                await asyncio.sleep(0.3)
            
            validated_plan = await self._validate_plan(plan)
            
            # ç¬¬äº”æ­¥ï¼šåˆ†æä»»åŠ¡å¤æ‚åº¦å’Œç±»å‹
            task_analysis = await self._analyze_task_complexity(user_input, validated_plan)
            
            task_plan = TaskPlan(
                user_input=user_input,
                task_type=task_analysis.get("task_type", task_type),
                complexity_level=task_analysis.get("complexity_level", "medium"),
                plan=validated_plan,
                status=TaskStatus.PLANNING
            )
            
            # å‘å°„è®¡åˆ’ç”Ÿæˆå®Œæˆäº‹ä»¶
            if self.event_emitter:
                await self.event_emitter.emit_plan_generated(
                    task_plan.task_id,
                    len(validated_plan.steps),
                    task_plan.task_type
                )
                await asyncio.sleep(0.5)
            
            logger.info(f"âœ… ä»»åŠ¡åˆ†æå®Œæˆï¼Œç”Ÿæˆ{len(validated_plan.steps)}ä¸ªæ‰§è¡Œæ­¥éª¤")
            await self._format_plan_output(task_plan)
            return task_plan
            
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡åˆ†æå¤±è´¥: {e}")
            if self.event_emitter:
                await self.event_emitter.emit_general_progress(
                    "analysis_failed",
                    f"ä»»åŠ¡åˆ†æå¤±è´¥: {str(e)}"
                )
            raise
    
    async def _analyze_requirements(self, user_input: str) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ·éœ€æ±‚ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è¿½é—®"""
        try:
            messages = [
                {"role": "system", "content": IMPROVED_REASON_SYSTEM_PROMPT},
                {"role": "user", "content": user_input}
            ]
            
            await self._stream_print("ğŸ¤” åˆ†ææ˜¯å¦éœ€è¦æ¾„æ¸…...")

            response = self.llm_client.beta.chat.completions.parse(
                    model=self.model_name,
                    messages=messages,
                    temperature=0.3,
                    response_format=TaskNeedClarification
                )
            response_text = json.loads(response.choices[0].message.content.strip())
            
            if response_text.get("need_clarification", False):
                questions_list = response_text.get("questions", [])
                if isinstance(questions_list, list):
                    valid_questions = [q.strip() for q in questions_list if isinstance(q, str) and q.strip() and ('?' in q or 'ï¼Ÿ' in q)]
                    return {
                        "needs_clarification": True,
                        "questions": valid_questions if valid_questions else ["è¯·æä¾›æ›´å¤šè¯¦ç»†ä¿¡æ¯ä»¥ä¾¿æˆ‘ä¸ºæ‚¨åˆ¶å®šè§£å†³æ–¹æ¡ˆã€‚"]
                    }
                else:
                    return {
                        "needs_clarification": True,
                        "questions": ["è¯·æä¾›æ›´å¤šè¯¦ç»†ä¿¡æ¯ä»¥ä¾¿æˆ‘ä¸ºæ‚¨åˆ¶å®šè§£å†³æ–¹æ¡ˆã€‚"]
                    }
            else:
                # ä¸éœ€è¦è¿½é—®çš„æƒ…å†µ
                return {"needs_clarification": False}
                
        except Exception as e:
            logger.error(f"éœ€æ±‚åˆ†æå¤±è´¥: {e}")
            return {"needs_clarification": False}
    
    async def _generate_plan(self, user_input: str) -> Plan:
        """ç”Ÿæˆå…·ä½“çš„æ‰§è¡Œè®¡åˆ’"""
        try:
            # è·å–å·¥å…·ä¿¡æ¯
            tools_info = self.tool_manager.get_tools_for_planning()
            
            # ç”Ÿæˆå·¥å…·çº¦æŸæç¤º
            tool_constraint = self.tool_manager.generate_tool_constraint_prompt()
            
            # æ ¼å¼åŒ–æç¤ºè¯
            formatted_prompt = PLAN_SYSTEM_PROMPT.format(
                tools=json.dumps(tools_info, ensure_ascii=False, indent=2),
                output_format=json.dumps(Plan.model_json_schema(), ensure_ascii=False, indent=2)
            ) + tool_constraint
            
            messages = [
                {"role": "system", "content": formatted_prompt},
                {"role": "user", "content": user_input}
            ]
            
            await self._stream_print("âš™ï¸ ç”Ÿæˆæ‰§è¡Œè®¡åˆ’...")
            # æ·»åŠ æµå¼è¾“å‡º
            response = self.llm_client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=0.1,
                stream=True
            )
            
            # å¤„ç†æµå¼å“åº”
            full_response = ""
            for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    await self._stream_print(content, end="")
                    full_response += content
            await self._stream_print()  # æ¢è¡Œ
            
            response_text = full_response.strip()
            
            # å°è¯•è§£æJSONå“åº”
            try:
                # æå–JSONéƒ¨åˆ†
                if "```json" in response_text:
                    json_start = response_text.find("```json") + 7
                    json_end = response_text.find("```", json_start)
                    json_text = response_text[json_start:json_end].strip()
                elif "{" in response_text:
                    json_start = response_text.find("{")
                    json_end = response_text.rfind("}") + 1
                    json_text = response_text[json_start:json_end]
                else:
                    raise ValueError("æœªæ‰¾åˆ°JSONæ ¼å¼çš„è®¡åˆ’")
                
                plan_data = json.loads(json_text)
                return Plan(**plan_data)
                
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"è§£æè®¡åˆ’JSONå¤±è´¥: {e}")
                # å¦‚æœè§£æå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤è®¡åˆ’
                return self._create_fallback_plan(user_input)
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆè®¡åˆ’å¤±è´¥: {e}")
            return self._create_fallback_plan(user_input)
    
    async def _validate_plan(self, plan: Plan) -> Plan:
        """éªŒè¯è®¡åˆ’ä¸­çš„å·¥å…·è°ƒç”¨ï¼ˆä¸ä¿®æ”¹å·¥å…·åç§°ï¼‰"""
        available_tools = self.tool_manager.get_available_tool_names()
        
        for step in plan.steps:
            # åªéªŒè¯å·¥å…·æ˜¯å¦å­˜åœ¨ï¼Œä¸è¿›è¡Œä¿®æ”¹
            if not self.tool_manager.is_tool_available(step.function_name):
                logger.warning(f"âš ï¸  å·¥å…·ä¸å­˜åœ¨: {step.function_name}ï¼Œå¯ç”¨å·¥å…·: {available_tools}")
                # å¯ä»¥åœ¨è¿™é‡Œè®°å½•è­¦å‘Šï¼Œä½†ä¸ä¿®æ”¹å·¥å…·åç§°
        
        return plan
    
    def _create_fallback_plan(self, user_input: str) -> Plan:
        """åˆ›å»ºåå¤‡è®¡åˆ’"""
        return Plan(steps=[
            Step(
                step_description=f"å¤„ç†ç”¨æˆ·è¯·æ±‚: {user_input}",
                function_name="generate_answer_tool",
                args={"query": user_input},
                is_final=True
            )
        ])
    
    async def _analyze_task_type(self, user_input: str) -> str:
        """åˆ†æä»»åŠ¡ç±»å‹"""
        try:
            messages = [
                {"role": "system", "content": "è¯·åˆ†æç”¨æˆ·è¾“å…¥çš„ä»»åŠ¡ç±»å‹"},
                {"role": "user", "content": user_input}
            ]
            
            await self._stream_print("ğŸ” åˆ†æä»»åŠ¡ç±»å‹...")
            # å¯¹äºç®€å•çš„åˆ†æä»»åŠ¡ï¼Œå…ˆå°è¯•æµå¼è¾“å‡º
            response = self.llm_client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=0.1,
                stream=True
            )
            
            # å¤„ç†æµå¼å“åº”
            full_response = ""
            for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    await self._stream_print(content, end="")
                    full_response += content
            await self._stream_print()  # æ¢è¡Œ
            
            # å¦‚æœéœ€è¦ç»“æ„åŒ–è¾“å‡ºï¼Œä½¿ç”¨éæµå¼æ–¹å¼
            response = self.llm_client.beta.chat.completions.parse(
                model=self.model_name,
                messages=messages,
                temperature=0.1,
                response_format=TaskType
            )
            response_text = response.choices[0].message.parsed
            return response_text.type
        except Exception as e:
            logger.error(f"ä»»åŠ¡ç±»å‹åˆ†æå¤±è´¥: {e}")
            return "é€šç”¨ä»»åŠ¡"
    
    async def _analyze_task_complexity(self, user_input: str, plan: Plan) -> Dict[str, str]:
        """åˆ†æä»»åŠ¡å¤æ‚åº¦å’Œç±»å‹"""
        step_count = len(plan.steps)
        
        # åŸºäºæ­¥éª¤æ•°é‡åˆ¤æ–­å¤æ‚åº¦
        if step_count == 1:
            complexity = "simple"
        elif step_count <= 3:
            complexity = "medium"
        else:
            complexity = "complex"
        
        # åŸºäºå¤§æ¨¡å‹åˆ¤æ–­ä»»åŠ¡ç±»å‹
        task_type = await self._analyze_task_type(user_input)
        
        return {
            "complexity_level": complexity,
            "task_type": task_type
        }
    
    async def _format_plan_output(self, task_plan: TaskPlan):
        """æ ¼å¼åŒ–å¹¶è¾“å‡ºä»»åŠ¡è®¡åˆ’"""
        try:
            await self._stream_print(f"\nğŸ“‹ ä»»åŠ¡åˆ†æå®Œæˆï¼")
            await self._stream_print(f"ğŸ“Š ä»»åŠ¡ç±»å‹: {task_plan.task_type}")
            await self._stream_print(f"âš¡ å¤æ‚åº¦: {task_plan.complexity_level}")
            await self._stream_print(f"ğŸ”§ æ‰§è¡Œæ­¥éª¤: {len(task_plan.plan.steps)}ä¸ª\n")
            
            for i, step in enumerate(task_plan.plan.steps, 1):
                await self._stream_print(f"æ­¥éª¤{i}: {step.step_description}")
                await self._stream_print(f"   å·¥å…·: {step.function_name}")
                if hasattr(step, 'args') and step.args:
                    await self._stream_print(f"   å‚æ•°: {step.args}")
                await self._stream_print("")
            
        except Exception as e:
            logger.warning(f"æ ¼å¼åŒ–è®¡åˆ’è¾“å‡ºå¤±è´¥: {e}")
    
    async def _detect_conversation(self, user_input: str) -> bool:
        """
        æ£€æµ‹ç”¨æˆ·è¾“å…¥æ˜¯å¦ä¸ºå¯¹è¯è€Œéä»»åŠ¡
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            
        Returns:
            bool: Trueè¡¨ç¤ºæ˜¯å¯¹è¯ï¼ŒFalseè¡¨ç¤ºæ˜¯ä»»åŠ¡
        """
        conversation_prompt = """
è¯·åˆ¤æ–­ç”¨æˆ·è¾“å…¥æ˜¯å¯¹è¯äº¤æµè¿˜æ˜¯å…·ä½“ä»»åŠ¡è¯·æ±‚ã€‚

å¯¹è¯äº¤æµç‰¹å¾ï¼ˆè¿”å›conversationï¼‰ï¼š
- é—®å€™è¯­ï¼šä½ å¥½ã€æ—©ä¸Šå¥½ã€æ™šå®‰ã€hiã€helloç­‰
- æ„Ÿè°¢è¡¨è¾¾ï¼šè°¢è°¢ã€æ„Ÿè°¢ã€å¤ªå¥½äº†ç­‰
- ç®€å•é—²èŠï¼šä»Šå¤©å¤©æ°”å¦‚ä½•ã€ä½ æ˜¯è°ã€ä½ å¥½å—ç­‰  
- ç³»ç»Ÿè¯¢é—®ï¼šä½ èƒ½åšä»€ä¹ˆã€ä½ çš„åŠŸèƒ½æ˜¯ä»€ä¹ˆç­‰
- çº¯ç²¹çš„æƒ…æ„Ÿè¡¨è¾¾æˆ–è¯„ä»·

ä»»åŠ¡è¯·æ±‚ç‰¹å¾ï¼ˆè¿”å›taskï¼‰ï¼š
- åŒ…å«æ˜ç¡®åŠ¨ä½œè¯ï¼šç”Ÿæˆã€åˆ›å»ºã€æœç´¢ã€åˆ†æã€è½¬æ¢ã€åˆ¶å®šã€å­¦ä¹ ã€å†™ã€åšç­‰
- æœ‰å…·ä½“ç›®æ ‡ï¼šå­¦ä¹ æŸä¸ªæŠ€èƒ½ã€åˆ¶å®šè®¡åˆ’ã€åˆ›å»ºæ–‡ä»¶ã€æœç´¢ä¿¡æ¯ç­‰
- è¯·æ±‚å¸®åŠ©å®Œæˆå…·ä½“äº‹æƒ…ï¼šå³ä½¿è¡¨è¿°ä¸å¤Ÿæ¸…æ™°ï¼Œä½†æœ‰æ˜ç¡®çš„åšäº‹æ„å›¾
- éœ€è¦äº§å‡ºç»“æœï¼šæ–‡ä»¶ã€è®¡åˆ’ã€åˆ†ææŠ¥å‘Šã€ä»£ç ç­‰

åˆ¤æ–­åŸåˆ™ï¼š
- åªæœ‰çº¯ç²¹çš„é—®å€™ã€æ„Ÿè°¢ã€é—²èŠæ‰æ˜¯å¯¹è¯
- ä»»ä½•å¸¦æœ‰"åšäº‹"æ„å›¾çš„éƒ½æ˜¯ä»»åŠ¡ï¼Œå³ä½¿æè¿°ä¸å¤Ÿè¯¦ç»†
- "æˆ‘æƒ³å­¦ä¹ ..."ã€"è¯·å¸®æˆ‘åˆ¶å®š..."ã€"æˆ‘éœ€è¦..."ç­‰éƒ½æ˜¯ä»»åŠ¡
- å®å¯è¯¯åˆ¤ä¸ºä»»åŠ¡ï¼Œä¹Ÿä¸è¦æŠŠä»»åŠ¡è¯¯åˆ¤ä¸ºå¯¹è¯

è¾“å…¥: {user_input}

è¯·ç›´æ¥å›ç­”: conversation æˆ– task
"""
        
        try:
            messages = [
                {"role": "system", "content": conversation_prompt.format(user_input=user_input)}
            ]
            
            response = self.llm_client.beta.chat.completions.parse(
                model=self.model_name,
                messages=messages,
                temperature=0.1,
                max_tokens=10,
                response_format=IsTaskOrConversation
            )

            result = response.choices[0].message.parsed
            return result.type == "conversation"
            
        except Exception as e:
            logger.error(f"å¯¹è¯æ£€æµ‹å¤±è´¥: {e}")
            # é»˜è®¤è®¤ä¸ºæ˜¯ä»»åŠ¡ï¼Œé¿å…è¯¯åˆ¤
            return False
    
    async def _generate_conversation_response(self, user_input: str) -> str:
        """
        ä¸ºå¯¹è¯ç±»å‹çš„è¾“å…¥ç”Ÿæˆå›å¤
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            
        Returns:
            str: å¯¹è¯å›å¤
        """
        conversation_system = """
ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚ç”¨æˆ·æ­£åœ¨ä¸ä½ è¿›è¡Œæ—¥å¸¸å¯¹è¯ï¼Œè¯·è‡ªç„¶åœ°å›åº”ã€‚
ä¿æŒç®€æ´ã€å‹å¥½å’Œæœ‰å¸®åŠ©çš„è¯­è°ƒã€‚
å¦‚æœç”¨æˆ·é—®å€™ï¼Œè¯·ç¤¼è²Œå›åº”ã€‚
å¦‚æœç”¨æˆ·æ„Ÿè°¢ï¼Œè¯·å®¢æ°”å›å¤ã€‚
å¦‚æœç”¨æˆ·è¯¢é—®ä½ çš„èƒ½åŠ›ï¼Œè¯·ç®€è¦ä»‹ç»ä½ å¯ä»¥å¸®åŠ©å®Œæˆçš„ä»»åŠ¡ç±»å‹ã€‚
"""
        
        try:
            messages = [
                {"role": "system", "content": conversation_system},
                {"role": "user", "content": user_input}
            ]
            
            response = self.llm_client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=0.7,
                max_tokens=150
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¯¹è¯å›å¤å¤±è´¥: {e}")
            return "æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„AIåŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ"
    
    async def refine_plan_with_feedback(self, task_plan: TaskPlan, user_feedback: str) -> TaskPlan:
        """æ ¹æ®ç”¨æˆ·åé¦ˆä¼˜åŒ–è®¡åˆ’"""
        try:
            logger.info("ğŸ”„ æ ¹æ®ç”¨æˆ·åé¦ˆä¼˜åŒ–è®¡åˆ’...")
            
            # ç»“åˆåŸå§‹éœ€æ±‚å’Œç”¨æˆ·åé¦ˆé‡æ–°ç”Ÿæˆè®¡åˆ’
            combined_input = f"{task_plan.user_input}\n\nç”¨æˆ·è¡¥å……è¦æ±‚: {user_feedback}"
            new_plan = await self._generate_plan(combined_input)
            
            # æ›´æ–°ä»»åŠ¡è®¡åˆ’
            task_plan.plan = new_plan
            task_plan.user_input = combined_input
            task_plan.requires_clarification = False
            task_plan.clarification_questions = []
            
            logger.info("âœ… è®¡åˆ’ä¼˜åŒ–å®Œæˆ")
            return task_plan
            
        except Exception as e:
            logger.error(f"âŒ è®¡åˆ’ä¼˜åŒ–å¤±è´¥: {e}")
            raise 
    
    async def _detect_task_improvement(self, user_input: str) -> bool:
        """
        æ£€æµ‹ç”¨æˆ·è¾“å…¥æ˜¯å¦ä¸ºå¯¹ä¸Šä¸€ä¸ªä»»åŠ¡çš„æ”¹è¿›è¯·æ±‚
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            
        Returns:
            bool: Trueè¡¨ç¤ºæ˜¯æ”¹è¿›è¯·æ±‚ï¼ŒFalseè¡¨ç¤ºä¸æ˜¯
        """
        improvement_prompt = """
è¯·åˆ¤æ–­ç”¨æˆ·è¾“å…¥æ˜¯å¦ä¸ºå¯¹ä¸Šä¸€ä¸ªä»»åŠ¡çš„æ”¹è¿›è¯·æ±‚ã€‚

æ”¹è¿›è¯·æ±‚ç‰¹å¾ï¼š
- ä½¿ç”¨"å¢åŠ "ã€"æ·»åŠ "ã€"ä¿®æ”¹"ã€"æ”¹è¿›"ã€"ä¼˜åŒ–"ã€"è°ƒæ•´"ç­‰è¯æ±‡
- æåˆ°"èƒŒæ™¯"ã€"æ ·å¼"ã€"é¢œè‰²"ã€"åŠŸèƒ½"ã€"ç•Œé¢"ç­‰å…·ä½“æ”¹è¿›ç‚¹
- ä½¿ç”¨"å¸®æˆ‘"ã€"ç»™æˆ‘"ã€"è®©å®ƒ"ç­‰æŒ‡ä»£æ€§è¯æ±‡ï¼Œæš—ç¤ºå¯¹ç°æœ‰å†…å®¹çš„ä¿®æ”¹
- æ²¡æœ‰æ˜ç¡®è¯´æ˜è¦åˆ›å»ºå…¨æ–°çš„ä¸œè¥¿
- è¯­å¢ƒæš—ç¤ºæ˜¯åœ¨ç°æœ‰åŸºç¡€ä¸Šè¿›è¡Œä¿®æ”¹

éæ”¹è¿›è¯·æ±‚ç‰¹å¾ï¼š
- æ˜ç¡®è¦æ±‚åˆ›å»ºå…¨æ–°çš„é¡¹ç›®æˆ–æ–‡ä»¶
- ä¸ä¸Šä¸€ä¸ªä»»åŠ¡å®Œå…¨æ— å…³çš„æ–°éœ€æ±‚
- ç‹¬ç«‹çš„é—®é¢˜æˆ–ä»»åŠ¡

ç¤ºä¾‹ï¼š
- "å¸®æˆ‘å¢åŠ ä¸€ä¸ªå¥½çœ‹çš„èƒŒæ™¯" â†’ æ”¹è¿›è¯·æ±‚
- "æ·»åŠ éŸ³æ•ˆ" â†’ æ”¹è¿›è¯·æ±‚  
- "ä¿®æ”¹é¢œè‰²ä¸ºè“è‰²" â†’ æ”¹è¿›è¯·æ±‚
- "ä¼˜åŒ–ç•Œé¢" â†’ æ”¹è¿›è¯·æ±‚
- "ç”Ÿæˆä¸€ä¸ªæ–°çš„Pythonç¨‹åº" â†’ éæ”¹è¿›è¯·æ±‚
- "æœç´¢æœºå™¨å­¦ä¹ èµ„æ–™" â†’ éæ”¹è¿›è¯·æ±‚

è¾“å…¥: {user_input}

è¯·ç›´æ¥å›ç­”: improvement æˆ– new_task
"""
        
        try:
            messages = [
                {"role": "system", "content": improvement_prompt.format(user_input=user_input)}
            ]
            
            response = self.llm_client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=0.1,
                max_tokens=10
            )
            
            result = response.choices[0].message.content.strip().lower()
            return "improvement" in result
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡æ”¹è¿›æ£€æµ‹å¤±è´¥: {e}")
            # é»˜è®¤è®¤ä¸ºä¸æ˜¯æ”¹è¿›è¯·æ±‚
            return False
    
    async def _handle_task_improvement(self, user_input: str, last_task: 'TaskPlan') -> 'TaskPlan':
        """
        å¤„ç†ä»»åŠ¡æ”¹è¿›è¯·æ±‚
        
        Args:
            user_input: ç”¨æˆ·çš„æ”¹è¿›è¦æ±‚
            last_task: ä¸Šä¸€ä¸ªå®Œæˆçš„ä»»åŠ¡
            
        Returns:
            TaskPlan: æ”¹è¿›ä»»åŠ¡çš„è®¡åˆ’
        """
        try:
            logger.info("ğŸ”§ å¤„ç†ä»»åŠ¡æ”¹è¿›è¯·æ±‚...")
            
            # å‘å°„ä»»åŠ¡åˆ†æå¼€å§‹äº‹ä»¶
            if self.event_emitter:
                await self.event_emitter.emit_task_analysis_start(f"æ”¹è¿›: {user_input}")
                await asyncio.sleep(0.5)
            
            # ç¡®ä¿å·¥å…·å·²åŠ è½½
            await self._load_tools()
            
            # è·å–ä¸Šä¸€ä¸ªä»»åŠ¡çš„æ–‡ä»¶ä¿¡æ¯
            last_task_info = ""
            if hasattr(last_task, 'plan') and last_task.plan.steps:
                last_task_info = f"ä¸Šä¸€ä¸ªä»»åŠ¡: {last_task.user_input}\n"
                if hasattr(last_task, 'generated_files') and last_task.generated_files:
                    last_task_info += f"å·²ç”Ÿæˆçš„æ–‡ä»¶: {', '.join(last_task.generated_files)}\n"
            
            # æ„å»ºæ”¹è¿›è¯·æ±‚çš„å®Œæ•´æè¿°
            improvement_request = f"""
åŸºäºä¸Šä¸€ä¸ªä»»åŠ¡çš„ç»“æœè¿›è¡Œæ”¹è¿›ï¼š

{last_task_info}
ç”¨æˆ·æ”¹è¿›è¦æ±‚: {user_input}

è¯·åœ¨ç°æœ‰æ–‡ä»¶çš„åŸºç¡€ä¸Šè¿›è¡Œä¿®æ”¹å’Œæ”¹è¿›ï¼Œä¸è¦é‡æ–°åˆ›å»ºå…¨æ–°çš„æ–‡ä»¶ã€‚
"""
            
            # å‘å°„è®¡åˆ’ç”Ÿæˆå¼€å§‹äº‹ä»¶
            if self.event_emitter:
                await self.event_emitter.emit_plan_generation_start("improvement")
                await asyncio.sleep(0.5)
            
            # ç”Ÿæˆæ”¹è¿›è®¡åˆ’
            plan = await self._generate_improvement_plan(improvement_request, last_task)
            
            # å‘å°„æ¯ä¸ªæ­¥éª¤ç”Ÿæˆäº‹ä»¶
            if self.event_emitter:
                for i, step in enumerate(plan.steps):
                    await self.event_emitter.emit_plan_step_generated(
                        i, len(plan.steps), step.step_description, step.function_name
                    )
                    await asyncio.sleep(0.2)
            
            # éªŒè¯è®¡åˆ’
            validated_plan = await self._validate_plan(plan)
            
            # åˆ›å»ºæ”¹è¿›ä»»åŠ¡è®¡åˆ’
            improvement_task = TaskPlan(
                user_input=f"æ”¹è¿›: {user_input}",
                task_type="ä»»åŠ¡æ”¹è¿›",
                complexity_level="medium",
                plan=validated_plan,
                status=TaskStatus.PLANNING,
                parent_task_id=last_task.task_id if hasattr(last_task, 'task_id') else None
            )
            
            # å‘å°„è®¡åˆ’ç”Ÿæˆå®Œæˆäº‹ä»¶
            if self.event_emitter:
                await self.event_emitter.emit_plan_generated(
                    improvement_task.task_id,
                    len(validated_plan.steps),
                    improvement_task.task_type
                )
                await asyncio.sleep(0.5)
            
            logger.info(f"âœ… ä»»åŠ¡æ”¹è¿›è®¡åˆ’ç”Ÿæˆå®Œæˆï¼ŒåŒ…å«{len(validated_plan.steps)}ä¸ªæ­¥éª¤")
            await self._format_plan_output(improvement_task)
            
            return improvement_task
            
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡æ”¹è¿›å¤„ç†å¤±è´¥: {e}")
            # å¦‚æœæ”¹è¿›å¤„ç†å¤±è´¥ï¼Œé™çº§ä¸ºæ™®é€šä»»åŠ¡å¤„ç†
            return await self._handle_as_normal_task(user_input)
    
    async def _generate_improvement_plan(self, improvement_request: str, last_task: 'TaskPlan') -> Plan:
        """
        ç”Ÿæˆæ”¹è¿›è®¡åˆ’
        
        Args:
            improvement_request: æ”¹è¿›è¯·æ±‚æè¿°
            last_task: ä¸Šä¸€ä¸ªä»»åŠ¡
            
        Returns:
            Plan: æ”¹è¿›è®¡åˆ’
        """
        improvement_prompt = """
# è§’è‰²ï¼š
ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡æ”¹è¿›ä¸“å®¶ï¼Œä¸“é—¨è´Ÿè´£åœ¨ç°æœ‰æˆæœåŸºç¡€ä¸Šè¿›è¡Œä¼˜åŒ–å’Œæ”¹è¿›ã€‚

# ä»»åŠ¡ï¼š
æ ¹æ®ç”¨æˆ·çš„æ”¹è¿›è¦æ±‚ï¼Œåˆ¶å®šå…·ä½“çš„æ”¹è¿›è®¡åˆ’ã€‚é‡ç‚¹æ˜¯åœ¨ç°æœ‰æ–‡ä»¶å’Œæˆæœçš„åŸºç¡€ä¸Šè¿›è¡Œä¿®æ”¹ï¼Œè€Œä¸æ˜¯é‡æ–°åˆ›å»ºã€‚

# æ”¹è¿›åŸåˆ™ï¼š
1. ä¼˜å…ˆä½¿ç”¨read_file_toolè¯»å–ç°æœ‰æ–‡ä»¶
2. ä½¿ç”¨file_generation_toolä¿®æ”¹ç°æœ‰æ–‡ä»¶æˆ–åˆ›å»ºå¢å¼ºç‰ˆæœ¬
3. ä¿æŒåŸæœ‰åŠŸèƒ½çš„åŒæ—¶æ·»åŠ æ–°åŠŸèƒ½
4. å¦‚æœéœ€è¦å›¾ç‰‡ç´ æï¼Œä½¿ç”¨image_generation_tool
5. åˆç†ä½¿ç”¨å…¶ä»–å·¥å…·æ¥å®Œå–„æ”¹è¿›

# å·¥å…·åˆ—è¡¨ï¼š
```
{tools}
```

# è¾“å‡ºæ ¼å¼ï¼š
```json
{output_format}
```

# æ³¨æ„äº‹é¡¹ï¼š
1. ç¬¬ä¸€æ­¥é€šå¸¸æ˜¯è¯»å–ç°æœ‰æ–‡ä»¶å†…å®¹
2. åŸºäºç°æœ‰å†…å®¹è¿›è¡Œæ”¹è¿›ï¼Œä¸è¦é‡æ–°å¼€å§‹
3. function_nameå¿…é¡»ä»å·¥å…·åˆ—è¡¨ä¸­é€‰æ‹©
4. æœ€åä¸€æ­¥è®¾ç½®is_finalä¸ºtrue
"""
        
        try:
            # è·å–å·¥å…·ä¿¡æ¯
            tools_info = self.tool_manager.get_tools_for_planning()
            tool_constraint = self.tool_manager.generate_tool_constraint_prompt()
            
            # æ ¼å¼åŒ–æç¤ºè¯
            formatted_prompt = improvement_prompt.format(
                tools=json.dumps(tools_info, ensure_ascii=False, indent=2),
                output_format=json.dumps(Plan.model_json_schema(), ensure_ascii=False, indent=2)
            ) + tool_constraint
            
            messages = [
                {"role": "system", "content": formatted_prompt},
                {"role": "user", "content": improvement_request}
            ]
            
            await self._stream_print("âš™ï¸ ç”Ÿæˆæ”¹è¿›è®¡åˆ’...")
            response = self.llm_client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=0.1,
                stream=True
            )
            
            # å¤„ç†æµå¼å“åº”
            full_response = ""
            for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    await self._stream_print(content, end="")
                    full_response += content
            await self._stream_print()  # æ¢è¡Œ
            
            response_text = full_response.strip()
            
            # è§£æJSONå“åº”
            try:
                if "```json" in response_text:
                    json_start = response_text.find("```json") + 7
                    json_end = response_text.find("```", json_start)
                    json_text = response_text[json_start:json_end].strip()
                elif "{" in response_text:
                    json_start = response_text.find("{")
                    json_end = response_text.rfind("}") + 1
                    json_text = response_text[json_start:json_end]
                else:
                    raise ValueError("æœªæ‰¾åˆ°JSONæ ¼å¼çš„è®¡åˆ’")
                
                plan_data = json.loads(json_text)
                return Plan(**plan_data)
                
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"è§£ææ”¹è¿›è®¡åˆ’JSONå¤±è´¥: {e}")
                return self._create_fallback_improvement_plan(improvement_request)
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ”¹è¿›è®¡åˆ’å¤±è´¥: {e}")
            return self._create_fallback_improvement_plan(improvement_request)
    
    def _create_fallback_improvement_plan(self, improvement_request: str) -> Plan:
        """åˆ›å»ºåå¤‡æ”¹è¿›è®¡åˆ’"""
        return Plan(steps=[
            Step(
                step_description=f"å¤„ç†æ”¹è¿›è¯·æ±‚: {improvement_request}",
                function_name="file_generation_tool",
                args={
                    "prompt": improvement_request,
                    "file_type": "py",
                    "file_name": "improved_version"
                },
                is_final=True
            )
        ])
    
    async def _handle_as_normal_task(self, user_input: str) -> 'TaskPlan':
        """å°†è¾“å…¥ä½œä¸ºæ™®é€šä»»åŠ¡å¤„ç†ï¼ˆæ”¹è¿›å¤±è´¥æ—¶çš„é™çº§æ–¹æ¡ˆï¼‰"""
        logger.info("ğŸ”„ æ”¹è¿›å¤„ç†å¤±è´¥ï¼Œé™çº§ä¸ºæ™®é€šä»»åŠ¡å¤„ç†")
        # æ¸…é™¤last_completed_taské¿å…é€’å½’
        self.last_completed_task = None
        return await self.analyze_task(user_input) 