import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from mcp.server.fastmcp import FastMCP
from tools.local_tools import web_search, read_file, file_generation, image_generation, data_chart
from openai import OpenAI
import mcp_server_fetch


# åˆ›å»º FastMCP æœåŠ¡å™¨
mcp = FastMCP(
    "marix", 
    debug=True,
    log_level="DEBUG",
    host="0.0.0.0",
    port=8001,
)

# æ³¨å†Œæ‰€æœ‰å·¥å…·åˆ°MCPæœåŠ¡å™¨
@mcp.tool()
def web_search_tool(query: str) -> dict:
    """ä½¿ç”¨searxngæœç´¢ç½‘ç»œä¿¡æ¯ï¼Œè·å–æœ€æ–°èµ„è®¯å’Œç›¸å…³é¡µé¢"""
    return web_search(query)

@mcp.tool()
def read_file_tool(file_path: str) -> str:
    """è¯»å–å„ç§ç±»å‹çš„æ–‡ä»¶å†…å®¹ï¼Œæ”¯æŒtxtã€pdfã€docxã€xlsxã€å›¾ç‰‡ç­‰æ ¼å¼"""
    return read_file(file_path)

@mcp.tool()
def file_generation_tool(prompt: str, file_type: str, file_name: str, output_dir: str = "./generated_files") -> dict:
    """æ ¹æ®æç¤ºè¯ç”Ÿæˆå„ç§ç±»å‹çš„æ–‡ä»¶ï¼Œå¦‚txtã€pyã€htmlã€mdã€jsonç­‰"""
    return file_generation(prompt, file_type, file_name, output_dir)

@mcp.tool()
def image_generation_tool(prompt: str, negative_prompt: str = "", size: str = "1024x1024", n: int = 1) -> dict:
    """æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆå›¾ç‰‡ï¼Œæ”¯æŒå„ç§å°ºå¯¸å’Œé£æ ¼"""
    return image_generation(prompt, negative_prompt, size, n)

@mcp.tool()
def data_chart_tool(data_description: str, chart_type: str = "bar") -> dict:
    """æ ¹æ®æ•°æ®æè¿°ç”Ÿæˆå›¾è¡¨"""
    return data_chart(data_description, chart_type)

@mcp.tool()
def generate_answer_tool(query: str) -> str:
    """ä½¿ç”¨AIå¤§æ¨¡å‹å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œè¿›è¡Œæ–‡æœ¬åˆ†æã€æ€»ç»“ã€ç¿»è¯‘ã€è§£é‡Šç­‰"""
    client = OpenAI(api_key="sk-proj-1234567890", base_url="http://180.153.21.76:17009/v1")
    try:
        print(f"ğŸ¤– AIæ­£åœ¨æ€è€ƒ: {query[:50]}...")
        response = client.chat.completions.create(
            model="Qwen-72B",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"}, 
                {"role": "user", "content": query}
            ],
            temperature=0.5,
            stream=True
        )
        
        # å¤„ç†æµå¼å“åº”
        full_response = ""
        for chunk in response:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                print(content, end="", flush=True)
                full_response += content
        print()  # æ¢è¡Œ
        
        return full_response
    except Exception as e:
        return f"AIå›ç­”å¤±è´¥: {str(e)}"

@mcp.tool()
def rhetorical_reason(user_query: str) -> str:
    """å¦‚æœç”¨æˆ·é—®é¢˜éœ€è¦è¿½é—®ï¼Œæ‰å¯ä»¥æ›´å¥½çš„è§£å†³ç”¨æˆ·é—®é¢˜ï¼Œåˆ™è°ƒç”¨è¯¥å·¥å…·"""
    REASON_SYSTEM_PROMPT = """
# è§’è‰²ï¼š

ä½ æ˜¯ä¸€åä¸“ä¸šçš„è§„åˆ’ä¸“å®¶ï¼Œä½ å¾ˆæ“…é•¿æ´æ‚‰ç”¨æˆ·éœ€æ±‚ã€‚
ä½ éœ€è¦è¿½é—®ç”¨æˆ·ä¸€äº›é—®é¢˜ï¼Œæ–¹ä¾¿åé¢ä½ æ›´å¥½çš„ä¸ºç”¨æˆ·è§£å†³é—®é¢˜ï¼Œåˆ¶å®šè§£å†³æ–¹æ¡ˆã€‚

# ä»»åŠ¡ï¼š

1. ç°åœ¨ä½ éœ€è¦æ ¹æ®ç”¨æˆ·é—®é¢˜ï¼Œè¿½é—®ä¸€äº›ç›¸å…³çš„é—®é¢˜ï¼Œæ–¹ä¾¿åé¢ä½ æ›´å¥½çš„ä¸ºç”¨æˆ·è§£å†³é—®é¢˜ï¼Œåˆ¶å®šè§£å†³æ–¹æ¡ˆã€‚
2. è¿½é—®çš„é—®é¢˜è¦å¥‘åˆç”¨æˆ·åŸå§‹é—®é¢˜ï¼Œä¸è¦è¿½é—®æ— å…³é—®é¢˜ã€‚
"""
    client = OpenAI(api_key="sk-proj-1234567890", base_url="http://180.153.21.76:17009/v1")
    
    print(f"ğŸ¤” åˆ†æç”¨æˆ·éœ€æ±‚: {user_query[:50]}...")
    response = client.chat.completions.create(
        model="Qwen-72B",
        messages=[
            {"role": "system", "content": REASON_SYSTEM_PROMPT}, 
            {"role": "user", "content": user_query}
        ],
        stream=True
    )
    
    # å¤„ç†æµå¼å“åº”
    full_response = ""
    for chunk in response:
        if chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            print(content, end="", flush=True)
            full_response += content
    print()  # æ¢è¡Œ
    
    return full_response

if __name__ == "__main__":
    mcp.run(transport="streamable-http")


# sk-8c40f79ea2044d0cbb9f7056aa5ec298